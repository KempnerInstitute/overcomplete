{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Overcomplete is a compact research library in Pytorch designed to study (Overcomplete)-Dictionary learning methods to extract concepts from large Vision models. In addition, this repository also introduces various visualization methods, attribution and metrics. However, Overcomplete emphasizes experimentation.</p>"},{"location":"#getting-started-with-overcomplete","title":"\ud83d\ude80 Getting Started with Overcomplete","text":"<p>Overcomplete requires Python 3.8 or newer and several dependencies, including Numpy. It supports both only Torch. Installation is straightforward with Pypi:</p> <pre><code>pip install overcomplete\n</code></pre> <p>With Overcomplete installed, you can dive into an optimisation based dictionary learning method to extract visual features or use the latest SAEs variant. The API is designed to be intuitive, requiring only a few hyperparameters to get started.</p> <p>Example usage:</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom overcomplete.sae import TopKSAE, train_sae\n\nActivations = torch.randn(N, d)\nsae = TopKSAE(d, nb_concepts=16_000, top_k=10, device='cuda')\n\ndataloader = DataLoader(TensorDataset(Activations), batch_size=1024)\noptimizer = torch.optim.Adam(sae.parameters(), lr=5e-4)\n\ndef criterion(x, x_hat, pre_codes, codes, dictionary):\n  mse = (x - x_hat).square().mean()\n  return mse\n\nlogs = train_sae(sae, dataloader, criterion, optimizer,\n                 nb_epochs=20, device='cuda')\n</code></pre>"},{"location":"#notebooks","title":"\ud83e\uddea Notebooks","text":"<ul> <li>Getting started: </li> <li>TopK, BatchTopK, JumpReLU, Vanilla SAE: </li> <li>Stable Dictionary with Archetypal-SAE: Coming soon</li> <li>Advanced metrics to study the solution of SAE: </li> <li>The visualization module: </li> <li>NMF, ConvexNMF and Semi-NMF: </li> <li>Modern Feature visualization to visualize concepts: Coming soon</li> </ul>"},{"location":"#credits","title":"\ud83d\udc4f Credits","text":"<p>This work has been made possible in part by the generous support provided by the Kempner Institute at Harvard University. The institute, established through a gift from the Chan Zuckerberg Initiative Foundation, is dedicated to advancing research in natural and artificial intelligence. The resources and commitment of the Kempner Institute have been instrumental in the development and completion of this project.</p>"},{"location":"#additional-resources","title":"Additional Resources","text":"<p>For a complete LLM implementation of the SAE, we strongly recommend exploring the following resources. The Sparsify library by EleutherAI provides a comprehensive toolset for implementing the SAE. The original TopK implementation is available through OpenAI's Sparse Autoencoder. Additionally, SAE Lens is an excellent resource, especially if you are interested in using the SAE-vis.</p>"},{"location":"#related-publications","title":"Related Publications","text":"<p>The Overcomplete framework has contributed to the following research publications.</p> <pre><code>@article{fel2025archetypal,\n  title     = {Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models},\n  author    = {Fel, Thomas and Lubana, Ekdeep Singh and Prince, Jacob S. and Kowal, Matthew and Boutin, Victor and Papadimitriou, Isabel and Wang, Binxu and Wattenberg, Martin and Ba, Demba and Konkle, Talia},\n  journal   = {arXiv preprint arXiv:2502.12892},\n  year      = {2025},\n  url       = {https://arxiv.org/abs/2502.12892}\n}\n</code></pre> <pre><code>@article{thasarathan2025universal,\n  title     = {Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment},\n  author    = {Thasarathan, Harrish and Forsyth, Julian and Fel, Thomas and Kowal, Matthew and Derpanis, Konstantinos},\n  journal   = {arXiv preprint arXiv:2502.03714},\n  year      = {2025},\n  url       = {https://arxiv.org/abs/2502.03714}\n}\n</code></pre> <pre><code>@article{hindupur2025projecting,\n  title     = {Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry},\n  author    = {Hindupur, Sai Sumedh R. and Lubana, Ekdeep Singh and Fel, Thomas and Ba, Demba},\n  journal   = {arXiv preprint arXiv:2503.01822},\n  year      = {2025},\n  url       = {https://arxiv.org/abs/2503.01822}\n}\n</code></pre>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Thomas Fel - tfel@g.harvard.edu, Kempner Research Fellow, Harvard University.</li> </ul>"},{"location":"metrics/","title":"Metrics for Dictionary Learning","text":"<p>The <code>overcomplete.metrics</code> module provides a collection of evaluation metrics designed for dictionary learning algorithms. These metrics help assess sparsity, reconstruction accuracy, and dictionary quality.</p>"},{"location":"metrics/#overview","title":"Overview","text":"<p>This module includes metrics for:</p> <ul> <li>Norm-based evaluations: L0, L1, L2, and Lp norms.</li> <li>Reconstruction losses: Absolute and relative errors.</li> <li>Sparsity metrics: Hoyer score, L1/L2 ratio, and Kappa-4.</li> <li>Dictionary similarity: Hungarian loss, cosine Hungarian loss, and collinearity.</li> <li>Distribution-based metrics: Wasserstein-1D and Fr\u00e9chet distance.</li> <li>Code analysis: Detecting dead codes and assessing sparsity.</li> </ul>"},{"location":"metrics/#example-usage-key-metrics","title":"Example Usage (Key Metrics)","text":"<pre><code>from overcomplete.metrics import (dead_codes, r2_score, hungarian_loss,\n                                 cosine_hungarian_loss, wasserstein_1d)\n# inputs\nx = torch.randn(100, 10)\nx_hat = torch.randn(100, 10)\n# dictionaries\ndict1 = torch.randn(512, 256)\ndict2 = torch.randn(512, 256)\n# concept values (codes)\ncodes = torch.randn(100, 512)\ncodes_2 = torch.randn(100, 512)\n\n# check for inactive dictionary elements\ndead_code_ratio = dead_codes(codes).mean()\n\n# compare dictionary structures\nhungarian_dist = hungarian_loss(dict1, dict2)\ncosine_hungarian_dist = cosine_hungarian_loss(dict1, dict2)\n\n# compute reconstruction quality\nr2 = r2_score(x, x_hat)\n# distrib. reconstruction quality\nwasserstein_dist = wasserstein_1d(x, x_hat)\n</code></pre>"},{"location":"metrics/#available-metrics","title":"Available Metrics","text":""},{"location":"metrics/#norm-based-metrics","title":"Norm-Based Metrics","text":"<ul> <li><code>l0(x)</code>, <code>l1(x)</code>, <code>l2(x)</code>, <code>lp(x, p)</code></li> <li><code>l1_l2_ratio(x)</code>: Ratio of L1 to L2 norm.</li> <li><code>hoyer(x)</code>: Normalized sparsity measure.</li> </ul>"},{"location":"metrics/#reconstruction-losses","title":"Reconstruction Losses","text":"<ul> <li><code>avg_l2_loss(x, x_hat)</code>, <code>avg_l1_loss(x, x_hat)</code></li> <li><code>relative_avg_l2_loss(x, x_hat)</code>, <code>relative_avg_l1_loss(x, x_hat)</code></li> <li><code>r2_score(x, x_hat)</code>: Measures reconstruction accuracy.</li> </ul>"},{"location":"metrics/#sparsity-metrics","title":"Sparsity Metrics","text":"<ul> <li><code>sparsity(x)</code>: Alias for <code>l0(x)</code>.</li> <li><code>sparsity_eps(x, threshold)</code>: L0 with an epsilon threshold.</li> <li><code>kappa_4(x)</code>: Kurtosis-based sparsity measure.</li> <li><code>dead_codes(x)</code>: Identifies unused codes in a dictionary.</li> </ul>"},{"location":"metrics/#dictionary-evaluation","title":"Dictionary Evaluation","text":"<ul> <li><code>hungarian_loss(dict1, dict2)</code>: Finds best-matching dictionary elements.</li> <li><code>cosine_hungarian_loss(dict1, dict2)</code>: Cosine distance-based Hungarian loss.</li> <li><code>dictionary_collinearity(dict)</code>: Measures collinearity in dictionary elements.</li> </ul>"},{"location":"metrics/#distribution-based-metrics","title":"Distribution-Based Metrics","text":"<ul> <li><code>wasserstein_1d(x1, x2)</code>: 1D Wasserstein-1 distance.</li> <li><code>frechet_distance(x1, x2)</code>: Fr\u00e9chet distance for distributions.</li> </ul> <p>For further details, refer to the module documentation.</p>"},{"location":"metrics/#l2","title":"<code>l2(v,\u00a0\u00a0 dims=None)</code>","text":"<p>Compute the L2 norm, across 'dims'. </p> <p>Parameters</p> <ul> <li> <p>v            : torch.Tensor </p> <ul> <li><p> Input tensor.</p> </li> </ul> </li> <li> <p>dims            : tuple, optional </p> <ul> <li><p> Dimensions over which to compute the L2 norm, by default None.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> L2 norm of v if dims=None else L2 norm across dims.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#l1","title":"<code>l1(v,\u00a0\u00a0 dims=None)</code>","text":"<p>Compute the L1 norm, across 'dims'. </p> <p>Parameters</p> <ul> <li> <p>v            : torch.Tensor </p> <ul> <li><p> Input tensor.</p> </li> </ul> </li> <li> <p>dims            : tuple, optional </p> <ul> <li><p> Dimensions over which to compute the L1 norm, by default None.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> L1 norm of v if dims=None else L1 for across dims.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#lp","title":"<code>lp(v,\u00a0\u00a0 p=0.5,\u00a0\u00a0 dims=None)</code>","text":"<p>Compute the Lp norm, across 'dims'. </p> <p>Parameters</p> <ul> <li> <p>v            : torch.Tensor </p> <ul> <li><p> Input tensor.</p> </li> </ul> </li> <li> <p>p            : float, optional </p> <ul> <li><p> Power of the norm, by default 0.5.</p> </li> </ul> </li> <li> <p>dims            : tuple, optional </p> <ul> <li><p> Dimensions over which to compute the Lp norm, by default None.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Lp norm of v if dims=None else Lp norm across dims.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#avg_l2_loss","title":"<code>avg_l2_loss(x,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x_hat)</code>","text":"<p>Compute the L2 loss, averaged across samples. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Original input tensor of shape (batch_size, d).</p> </li> </ul> </li> <li> <p>x_hat            : torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, d).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>float </p> <ul> <li><p> Average L2 loss per sample.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#avg_l1_loss","title":"<code>avg_l1_loss(x,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x_hat)</code>","text":"<p>Compute the L1 loss, averaged across samples. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Original input tensor of shape (batch_size, d).</p> </li> </ul> </li> <li> <p>x_hat            : torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, d).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>float </p> <ul> <li><p> Average L1 loss per sample.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#relative_avg_l2_loss","title":"<code>relative_avg_l2_loss(x,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x_hat,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 epsilon=1e-06)</code>","text":"<p>Compute the relative reconstruction loss, average across samples. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Original input tensor of shape (batch_size, d).</p> </li> </ul> </li> <li> <p>x_hat            : torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, d).</p> </li> </ul> </li> <li> <p>epsilon            : float, optional </p> <ul> <li><p> Small value to avoid division by zero, by default 1e-6.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>float </p> <ul> <li><p> Average relative L2 loss per sample.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#relative_avg_l1_loss","title":"<code>relative_avg_l1_loss(x,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x_hat,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 epsilon=1e-06)</code>","text":"<p>Compute the relative reconstruction loss, average across samples. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Original input tensor of shape (batch_size, d).</p> </li> </ul> </li> <li> <p>x_hat            : torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, d).</p> </li> </ul> </li> <li> <p>epsilon            : float, optional </p> <ul> <li><p> Small value to avoid division by zero, by default 1e-6.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>float </p> <ul> <li><p> Average relative L1 loss per sample.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#l0","title":"<code>l0(x,\u00a0\u00a0 dims=None)</code>","text":"<p>Compute the average number of zero elements. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor.</p> </li> </ul> </li> <li> <p>dims            : tuple, optional </p> <ul> <li><p> Dimensions over which to compute the sparsity, by default None.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Average sparsity if dims=None else sparsity across dims.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#l0","title":"<code>l0(x,\u00a0\u00a0 dims=None)</code>","text":"<p>Compute the average number of zero elements. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor.</p> </li> </ul> </li> <li> <p>dims            : tuple, optional </p> <ul> <li><p> Dimensions over which to compute the sparsity, by default None.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Average sparsity if dims=None else sparsity across dims.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#l1_l2_ratio","title":"<code>l1_l2_ratio(x,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dims=-1)</code>","text":"<p>Compute the L1/L2 ratio of a tensor. By default, the ratio is computed across the last dimension. This score is a useful metric to evaluate the sparsity of a tensor. It is however sensitive to the dimensions of the data, for an unbiased metric, consider using the Hoyer score. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor.</p> </li> </ul> </li> <li> <p>dims            : tuple, optional </p> <ul> <li><p> Dimensions over which to compute the ratio, by default -1.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> the l1/l2 ratio.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#hoyer","title":"<code>hoyer(x)</code>","text":"<p>Compute the Hoyer sparsity of a tensor. The hoyer score include the dimension normalization factor. A score of 1 indicates a perfectly sparse representation, while a score of 0 indicates a dense representation. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> A 2D tensor of shape (batch_size, d).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor (batch_size,) </p> <ul> <li><p> Hoyer sparsity for each vector in the batch.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#kappa_4","title":"<code>kappa_4(x)</code>","text":"<p>Compute the Kappa-4 sparsity of a tensor. The Kappa-4 score is a metric to evaluate the sparsity of a distribution. It is the kurtosis, which measure the \"peakedness\" of a distribution. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor.</p> </li> </ul> </li> <li> <p>dims            : tuple, optional </p> <ul> <li><p> Dimensions over which to compute the ratio, by default -1.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> the Kappa-4 sparsity.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#r2_score","title":"<code>r2_score(x,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x_hat)</code>","text":"<p>Compute the R^2 score (coefficient of determination) for the reconstruction. A score of 1 indicates a perfect reconstruction while a score of 0 indicates that the reconstruction is as good as the mean. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Original input tensor of shape (batch_size, d).</p> </li> </ul> </li> <li> <p>x_hat            : torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, d).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>float </p> <ul> <li><p> R^2 score.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#dead_codes","title":"<code>dead_codes(z)</code>","text":"<p>Check for codes that never fire and return the percentage of codes that never fire. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, num_codes).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Tensor indicating which codes are dead.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#hungarian_loss","title":"<code>hungarian_loss(dictionary1,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dictionary2,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 p_norm=2)</code>","text":"<p>Compute the Hungarian loss between two dictionaries. </p> <p>Parameters</p> <ul> <li> <p>dictionary1            : torch.Tensor </p> <ul> <li><p> First dictionary tensor of shape (num_codes, dim).</p> </li> </ul> </li> <li> <p>dictionary2            : torch.Tensor </p> <ul> <li><p> Second dictionary tensor of shape (num_codes, dim).</p> </li> </ul> </li> <li> <p>p_norm            : int, optional </p> <ul> <li><p> Norm to use for computing the distance, by default 2.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>float </p> <ul> <li><p> Hungarian loss.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#cosine_hungarian_loss","title":"<code>cosine_hungarian_loss(dictionary1,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dictionary2)</code>","text":"<p>Compute the cosine Hungarian loss between two dictionaries. A score of 0 indicates that the two dictionaries are identical up to a permutation. A score of 'dim' indicates that the two dictionaries are orthogonal. To have a normalized score, we recommend to divide the score by the dimension of the dictionary. </p> <p>Parameters</p> <ul> <li> <p>dictionary1            : torch.Tensor </p> <ul> <li><p> First dictionary tensor of shape (num_codes, dim).</p> </li> </ul> </li> <li> <p>dictionary2            : torch.Tensor </p> <ul> <li><p> Second dictionary tensor of shape (num_codes, dim).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>float </p> <ul> <li><p> Cosine Hungarian loss.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#dictionary_collinearity","title":"<code>dictionary_collinearity(dictionary)</code>","text":"<p>Compute the collinearity of a dictionary. </p> <p>Parameters</p> <ul> <li> <p>dictionary            : torch.Tensor </p> <ul> <li><p> Dictionary tensor of shape (num_codes, dim).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>max_collinearity            : float </p> <ul> <li><p> Maximum collinearity across dictionary elements (non diagonal).</p> </li> </ul> </li> <li> <p>cosine_similarity_matrix            : torch.Tensor </p> <ul> <li><p> Matrix of cosine similarities across dictionary elements.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#wasserstein_1d","title":"<code>wasserstein_1d(x1,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x2)</code>","text":"<p>Compute the 1D Wasserstein-1 distance between two sets of codes and average across dimensions. </p> <p>Parameters</p> <ul> <li> <p>x1            : torch.Tensor </p> <ul> <li><p> First set of samples of shape (num_samples, d).</p> </li> </ul> </li> <li> <p>x2            : torch.Tensor </p> <ul> <li><p> Second set of samples of shape (num_samples, d).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Wasserstein distance.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#frechet_distance","title":"<code>frechet_distance(x1,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x2)</code>","text":"<p>Compute the Fr\u00e9chet distance (Wasserstein-2 distance) between two sets of activations. Assume that the activations are normally distributed. </p> <p>Parameters</p> <ul> <li> <p>x1            : torch.Tensor </p> <ul> <li><p> First set of samples of shape (num_samples, d).</p> </li> </ul> </li> <li> <p>x2            : torch.Tensor </p> <ul> <li><p> Second set of samples of shape (num_samples, d).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Fr\u00e9chet distance.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#codes_correlation_matrix","title":"<code>codes_correlation_matrix(codes)</code>","text":"<p>Compute the correlation matrix of codes. </p> <p>Parameters</p> <ul> <li> <p>codes            : torch.Tensor </p> <ul> <li><p> Codes tensor of shape (batch_size, num_codes).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>max_corr            : float </p> <ul> <li><p> Maximum correlation across codes (non diagonal).</p> </li> </ul> </li> <li> <p>corrs            : torch.Tensor </p> <ul> <li><p> Correlation matrix of codes.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"metrics/#energy_of_codes","title":"<code>energy_of_codes(codes,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dictionary)</code>","text":"<p>Compute the energy of codes given a dictionary. for example, with X input sample, Z the codes and D the dictionary: X = ZD, Energy(Z) = || E[Z]D ||^2 and correspond to the average energy the codes bring to the reconstruction. </p> <p>Parameters</p> <ul> <li> <p>codes            : torch.Tensor </p> <ul> <li><p> Codes tensor of shape (batch_size, num_codes).</p> </li> </ul> </li> <li> <p>dictionary            : torch.Tensor </p> <ul> <li><p> Dictionary tensor of shape (num_codes, dim).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Energy of codes, one per codes dimension.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"visualization/","title":"Visualization Modules","text":"<p>The <code>overcomplete.visualization</code> module provides a set of tools for analyzing and visualizing top-concept activations in batches of images. These tools help for understanding which part of an images contribute for some concept.</p>"},{"location":"visualization/#overview","title":"Overview","text":"<p>The visualization module includes functions for: - Overlaying heatmaps onto images to highlight top-concept activations. - Displaying the most representative images for a given concept. - Applying contour visualizations to emphasize highly activating regions. - Zooming into the hottest points of a heatmap. - Highlighting evidence areas using percentile-based heatmap thresholding.</p>"},{"location":"visualization/#example-usage","title":"Example Usage","text":"<pre><code>from overcomplete.visualization import (overlay_top_heatmaps,\nevidence_top_images, zoom_top_images, contour_top_image)\n# lets imagine we have 100 images and 10k concepts maps\n# and we want to visualize concept 3\nimages = torch.randn(100, 3, 256, 256)\nheatmaps = torch.randn(100, 14, 14, 10_000)\n\n# heatmap + transparency (recommended)\noverlay_top_heatmaps(images, heatmaps, concept_id=3,\n                     cmap='jet', alpha=0.35)\n# transparency based\nevidence_top_images(images, heatmaps, concept_id=3)\n# zoom into max activating crops\nzoom_top_images(images, heatmaps, concept_id=3, zoom_size=100)\n# contour of most important part (boundary)\ncontour_top_image(images, heatmaps, concept_id=3)\n</code></pre> <p>For more details, see the module in <code>Overcomplete.visualization</code>.</p>"},{"location":"visualization/#overlay_top_heatmaps","title":"<code>overlay_top_heatmaps(images,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 heatmaps,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 concept_id,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 cmap=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha=0.35)</code>","text":"<p>Visualize the top activating image for a concepts and overlay the associated heatmap. </p> <p>Parameters</p> <ul> <li> <p>images            : torch.Tensor or PIL.Image or np.ndarray </p> <ul> <li><p> Batch of input images of shape (batch_size, channels, height, width).</p> </li> </ul> </li> <li> <p>z_heatmaps            : torch.Tensor or np.ndarray </p> <ul> <li><p> Batch of heatmaps corresponding to the input images of shape (batch_size, height, width, num_concepts).</p> </li> </ul> </li> <li> <p>concept_id            : int </p> <ul> <li><p> Index of the concept to visualize.</p> </li> </ul> </li> <li> <p>cmap            : str, optional </p> <ul> <li><p> Colormap for the heatmap, by default 'jet'.</p> </li> </ul> </li> <li> <p>alpha            : float, optional </p> <ul> <li><p> Transparency of the heatmap overlay, by default 0.35.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"visualization/#evidence_top_images","title":"<code>evidence_top_images(images,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 heatmaps,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 concept_id,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 percentiles=None)</code>","text":"<p>Visualize the top activating image for a concept and highlight the top activating pixels. </p> <p>Parameters</p> <ul> <li> <p>images            : torch.Tensor or PIL.Image or np.ndarray </p> <ul> <li><p> Batch of input images of shape (batch_size, channels, height, width).</p> </li> </ul> </li> <li> <p>heatmaps            : torch.Tensor or np.ndarray </p> <ul> <li><p> Batch of heatmaps corresponding to the input images of shape (batch_size, height, width, num_concepts).</p> </li> </ul> </li> <li> <p>concept_id            : int </p> <ul> <li><p> Index of the concept to visualize.</p> </li> </ul> </li> <li> <p>percentiles            : list of int, optional </p> <ul> <li><p> List of percentiles to highlight, by default None.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"visualization/#zoom_top_images","title":"<code>zoom_top_images(images,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 heatmaps,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 concept_id,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 zoom_size=100)</code>","text":"<p>Zoom into the hottest point in the heatmaps for a specific concept. </p> <p>Parameters</p> <ul> <li> <p>images            : torch.Tensor or PIL.Image or np.ndarray </p> <ul> <li><p> Batch of input images of shape (batch_size, channels, height, width).</p> </li> </ul> </li> <li> <p>heatmaps            : torch.Tensor or np.ndarray </p> <ul> <li><p> Batch of heatmaps corresponding to the input images of shape (batch_size, height, width, num_concepts).</p> </li> </ul> </li> <li> <p>concept_id            : int </p> <ul> <li><p> Index of the concept to visualize.</p> </li> </ul> </li> <li> <p>zoom_size            : int, optional </p> <ul> <li><p> Size of the zoomed area around the hottest point, by default 100.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"visualization/#contour_top_image","title":"<code>contour_top_image(images,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 heatmaps,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 concept_id,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 percentiles=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 cmap='viridis',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 linewidth=1.0)</code>","text":"<p>Contour the best images for a specific concept using heatmap percentiles. </p> <p>Parameters</p> <ul> <li> <p>images            : torch.Tensor or PIL.Image or np.ndarray </p> <ul> <li><p> Batch of input images of shape (batch_size, channels, height, width).</p> </li> </ul> </li> <li> <p>heatmaps            : torch.Tensor or np.ndarray </p> <ul> <li><p> Batch of heatmaps corresponding to the input images of shape (batch_size, height, width, num_concepts).</p> </li> </ul> </li> <li> <p>concept_id            : int </p> <ul> <li><p> Index of the concept to visualize.</p> </li> </ul> </li> <li> <p>percentiles            : list of int, optional </p> <ul> <li><p> List of percentiles to contour, by default [70].</p> </li> </ul> </li> <li> <p>cmap            : str, optional </p> <ul> <li><p> Colormap for the contours, by default \"viridis\".</p> </li> </ul> </li> <li> <p>linewidth            : float, optional </p> <ul> <li><p> Width of the contour lines, by default 1.0.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/","title":"Convex Non-Negative Matrix Factorization (Convex NMF)","text":"<p>Convex Non-Negative Matrix Factorization (Convex NMF) is a variation of NMF that constrains the dictionary (<code>D</code>) to be a convex combination of input data (<code>A</code>). This ensures that learned basis components are directly interpretable in terms of the original data and the dictionary elements to be in the conical hull of the data.</p> <p>where:</p> <ul> <li><code>A</code> (Data Matrix): Input data, shape <code>(n_samples, n_features)</code>.</li> <li><code>Z</code> (Codes Matrix): Latent representation, shape <code>(n_samples, nb_concepts)</code>, constrained to be non-negative.</li> <li><code>W</code> (Coefficient Matrix): Convex coefficients, shape <code>(nb_concepts, n_samples)</code>, constrained to be non-negative. The dictionary is computed as <code>D = W A</code>.</li> </ul>"},{"location":"optimization/convexnmf/#basic-usage","title":"Basic Usage","text":"<pre><code>from overcomplete import ConvexNMF\n\n# define a Convex NMF model with 10k concepts using\n# the multiplicative update solver\nconvex_nmf = ConvexNMF(nb_concepts=10_000, solver='mu')\n\n# fit the model to input data A\nZ, D = convex_nmf.fit(A)\n\n# encode new data\nZ = convex_nmf.encode(A)\n# decode (reconstruct) data from codes\nA_hat = convex_nmf.decode(Z)\n</code></pre>"},{"location":"optimization/convexnmf/#solvers","title":"Solvers","text":"<p>The Convex NMF module supports different optimization strategies: - MU (Multiplicative Updates) - Standard Convex NMF update rule. - PGD (Projected Gradient Descent) - Allows for sparsity control via L1 penalty on <code>Z</code>.</p> <p>For further details, we encourage you to check the original references <sup>1</sup>.</p>"},{"location":"optimization/convexnmf/#ConvexNMF","title":"<code>ConvexNMF</code>","text":"<p>PyTorch Convex NMF-based Dictionary Learning model. </p>"},{"location":"optimization/convexnmf/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 tol=0.0001,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 strict_convex=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 solver='pgd',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 verbose=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 l1_penalty=0.0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 **kwargs)</code>","text":"<p>Parameters</p> <ul> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of components to learn.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device to use for tensor computations, by default 'cpu'.</p> </li> </ul> </li> <li> <p>tol            : float, optional </p> <ul> <li><p> Tolerance value for the stopping criterion, by default 1e-4.</p> </li> </ul> </li> <li> <p>strict_convex            : bool, optional </p> <ul> <li><p> Whether to enforce the convexity constraint, by default False.</p> </li> </ul> </li> <li> <p>solver            : str, optional </p> <ul> <li><p> Optimization solver to use, either 'mu' (Multiplicative Update) or 'pgd' like method, by default 'mu'.</p> </li> </ul> </li> <li> <p>verbose            : bool, optional </p> <ul> <li><p> Whether to print optimization information, by default False.</p> </li> </ul> </li> <li> <p>l1_penalty            : float, optional </p> <ul> <li><p> L1 penalty coefficient, by default 0.0. Only used with the 'pgd' solver.</p> </li> </ul> </li> </ul>"},{"location":"optimization/convexnmf/#init_random_z","title":"<code>init_random_z(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A)</code>","text":"<p>Initialize the codes Z using non negative random values. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Initialized codes tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Return the learned dictionary components from Convex NMF. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Dictionary components D = A W.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#encode","title":"<code>encode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_iter=300,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 tol=None)</code>","text":"<p>Encode the input tensor (the data) using Convex NMF. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (n_samples, n_features).</p> </li> </ul> </li> <li> <p>max_iter            : int, optional </p> <ul> <li><p> Maximum number of iterations, by default 300.</p> </li> </ul> </li> <li> <p>tol            : float, optional </p> <ul> <li><p> Tolerance value for the stopping criterion, by default the value set at initialization.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Encoded features (the codes Z).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#sanitize_np_input","title":"<code>sanitize_np_input(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Ensure the input tensor is a numpy array of shape (batch_size, dims). Convert from pytorch tensor or DataLoader if necessary. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor or Iterable </p> <ul> <li><p> Input tensor of shape (batch_size, dims).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Sanitized input tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#fit","title":"<code>fit(self,\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0 max_iter=500)</code>","text":"<p>Fit the Convex NMF model to the input data. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (n_samples, n_features).</p> </li> </ul> </li> <li> <p>max_iter            : int, optional </p> <ul> <li><p> Maximum number of iterations, by default 500.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#init_semi_nmf","title":"<code>init_semi_nmf(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_snmf_iters=100)</code>","text":"<p>Initialize the Convex NMF model using Semi-NMF. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (n_samples, n_features).</p> </li> </ul> </li> <li> <p>max_snmf_iters            : int, optional </p> <ul> <li><p> Maximum number of iterations for Semi-NMF, by default 100.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Initialized codes tensor.</p> </li> </ul> </li> <li> <p>D            : torch.Tensor </p> <ul> <li><p> Initialized dictionary tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#init_random_w","title":"<code>init_random_w(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A)</code>","text":"<p>Initialize the coefficient matrix W with non-negative random values. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (n_samples, n_features).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>W            : torch.Tensor </p> <ul> <li><p> Initialized coefficient tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#sanitize_np_codes","title":"<code>sanitize_np_codes(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Ensure the codes tensor (Z) is a numpy array of shape (batch_size, nb_concepts). Convert from pytorch tensor or DataLoader if necessary. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Encoded tensor (the codes) of shape (batch_size, nb_concepts).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Sanitized codes tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/convexnmf/#decode","title":"<code>decode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Z)</code>","text":"<p>Decode the input tensor (the codes) using Convex NMF. </p> <p>Parameters</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Codes tensor of shape (n_samples, nb_concepts).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Decoded output (the approximation of A).</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Convex and Semi-Nonnegative Matrix Factorizations by Ding, Li, and Jordan (2008).\u00a0\u21a9</p> </li> </ol>"},{"location":"optimization/nmf/","title":"Non-Negative Matrix Factorization (NMF)","text":"<p>Non-Negative Matrix Factorization (NMF) is a factorization method that decomposes a non-negative matrix into two lower-rank non-negative matrices. This technique is widely used in feature learning, dictionary learning, and dimensionality reduction, particularly in neural network activations.</p> <p>The model consists of:</p> <ul> <li>Input Matrix (<code>A</code>): The pattern of activations in a neural network, shaped as <code>(batch_size, n_features)</code>.</li> <li>Codes (<code>Z</code>): The representation of data in terms of discovered concepts, shaped as <code>(batch_size, nb_concepts)</code>.</li> <li>Dictionary (<code>D</code>): A learned basis of concepts, shaped as <code>(nb_concepts, n_features)</code>.</li> </ul>"},{"location":"optimization/nmf/#basic-usage","title":"Basic Usage","text":"<pre><code>from overcomplete import NMF\n\n# define an NMF model with 10k concepts using the HALS solver\nnmf = NMF(nb_concepts=10_000, solver='hals')\n\n# fit the model to input activations A\nZ, D = nmf.fit(A)\n\n# encode new data\nZ = nmf.encode(A)\n# decode (reconstruct) data from codes\nA_hat = nmf.decode(Z)\n</code></pre>"},{"location":"optimization/nmf/#solvers","title":"Solvers","text":"<p>The NMF module supports different optimization strategies: - HALS (Hierarchical Alternating Least Squares) - Efficient for large-scale problems. - MU (Multiplicative Updates) - Standard NMF update rule. - ANLS (Alternating Non-Negative Least Squares) - Robust least squares optimization. - PGD (Projected Gradient Descent) - Suitable for constrained optimization.</p> <p>With HALS usually providing best reconstruction.</p>"},{"location":"optimization/nmf/#NMF","title":"<code>NMF</code>","text":"<p>Torch NMF-based Dictionary Learning model. </p>"},{"location":"optimization/nmf/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 solver='hals',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 tol=0.0001,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 verbose=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 **kwargs)</code>","text":"<p>Parameters</p> <ul> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of components to learn.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device to use for tensor computations, by default 'cpu'</p> </li> </ul> </li> <li> <p>solver            : str, optional </p> <ul> <li><p> Optimization algorithm to use, by default 'hals'. Can be one of: - 'hals': Hierarchical Alternating Least Squares - 'mu': Multiplicative update rules - 'pgd': Projected Gradient Descent - 'anls': Alternating Non-negative Least Squares</p> </li> </ul> </li> <li> <p>tol            : float, optional </p> <ul> <li><p> Tolerance value for the stopping criterion, by default 1e-4.</p> </li> </ul> </li> <li> <p>verbose            : bool, optional </p> <ul> <li><p> Whether to display a progress bar, by default False.</p> </li> </ul> </li> </ul>"},{"location":"optimization/nmf/#init_random_z","title":"<code>init_random_z(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A)</code>","text":"<p>Initialize the codes Z using non negative random values. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Initialized codes tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#init_nndsvda","title":"<code>init_nndsvda(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A)</code>","text":"<p>Initialize the dictionary and the code using Sklearn NNDsvdA algorithm. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Initialized codes tensor.</p> </li> </ul> </li> <li> <p>D            : torch.Tensor </p> <ul> <li><p> Initialized dictionary tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Return the learned dictionary components from NMF. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Dictionary components.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#encode","title":"<code>encode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_iter=300,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 tol=None)</code>","text":"<p>Encode the input tensor (the activations) using NMF. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor or Iterable </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> <li> <p>max_iter            : int, optional </p> <ul> <li><p> Maximum number of iterations, by default 100.</p> </li> </ul> </li> <li> <p>tol            : float, optional </p> <ul> <li><p> Tolerance value for the stopping criterion, by default the value set in the constructor.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Encoded features (the codes).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#sanitize_np_input","title":"<code>sanitize_np_input(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Ensure the input tensor is a numpy array of shape (batch_size, dims). Convert from pytorch tensor or DataLoader if necessary. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor or Iterable </p> <ul> <li><p> Input tensor of shape (batch_size, dims).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Sanitized input tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#fit","title":"<code>fit(self,\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0 max_iter=500)</code>","text":"<p>Fit the NMF model to the input data. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor or Iterable </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> <li> <p>max_iter            : int, optional </p> <ul> <li><p> Maximum number of iterations, by default 500.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#init_random_d","title":"<code>init_random_d(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A)</code>","text":"<p>Initialize the dictionary D using non negative random values. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>D            : torch.Tensor </p> <ul> <li><p> Initialized dictionary tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#sanitize_np_codes","title":"<code>sanitize_np_codes(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Ensure the codes tensor (Z) is a numpy array of shape (batch_size, nb_concepts). Convert from pytorch tensor or DataLoader if necessary. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Encoded tensor (the codes) of shape (batch_size, nb_concepts).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Sanitized codes tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/nmf/#decode","title":"<code>decode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Z)</code>","text":"<p>Decode the input tensor (the codes) using NMF. </p> <p>Parameters</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Encoded tensor (the codes) of shape (batch_size, nb_concepts).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Decoded output (the activations).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/","title":"Semi-Nonnegative Matrix Factorization (Semi-NMF)","text":"<p>Semi-Nonnegative Matrix Factorization (Semi-NMF) is a variant of NMF that relaxes the constraint on the code matrix (<code>Z</code>), allowing negative values while keeping the dictionary (<code>D</code>) nonnegative. The model consists of:</p> <ul> <li>Input Matrix (<code>A</code>): The pattern of activations in a neural network, shaped as <code>(batch_size, n_features)</code>.</li> <li>Codes (<code>Z</code>): The representation of data in terms of discovered concepts, shaped as <code>(batch_size, nb_concepts)</code>, and can take negative values.</li> <li>Dictionary (<code>D</code>): A learned basis of concepts, shaped as <code>(nb_concepts, n_features)</code>, constrained to be nonnegative.</li> </ul>"},{"location":"optimization/seminmf/#basic-usage","title":"Basic Usage","text":"<pre><code>from overcomplete import SemiNMF\n\n# define a Semi-NMF model with 10k concepts using\n# the multiplicative update solver\nsemi_nmf = SemiNMF(nb_concepts=10_000, solver='mu')\n\n# fit the model to input activations A\nZ, D = semi_nmf.fit(A)\n\n# encode new data\nZ = semi_nmf.encode(A)\n# decode (reconstruct) data from codes\nA_hat = semi_nmf.decode(Z)\n</code></pre>"},{"location":"optimization/seminmf/#solvers","title":"Solvers","text":"<p>The Semi-NMF module supports different optimization strategies: - MU (Multiplicative Updates) - Standard Semi-NMF update rule. - PGD (Projected Gradient Descent) - Allows for sparsity control via L1 penalty on <code>Z</code>.</p> <p>For further details, we encourage you to check the original references <sup>1</sup>.</p>"},{"location":"optimization/seminmf/#SemiNMF","title":"<code>SemiNMF</code>","text":"<p>Torch Semi-NMF-based Dictionary Learning model. </p>"},{"location":"optimization/seminmf/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 solver='mu',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 tol=0.0001,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 l1_penalty=0.0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 verbose=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 **kwargs)</code>","text":"<p>Parameters</p> <ul> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of components to learn.</p> </li> </ul> </li> <li> <p>solver            : str, optional </p> <ul> <li><p> Solver to use, by default 'mu'. Possible values are 'mu' (multiplicative update) and 'pgd' (projected gradient descent), 'pgd' allows for sparsity penalty.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device to use for tensor computations, by default 'cpu'</p> </li> </ul> </li> <li> <p>tol            : float, optional </p> <ul> <li><p> Tolerance value for the stopping criterion, by default 1e-4.</p> </li> </ul> </li> <li> <p>l1_penalty            : float, optional </p> <ul> <li><p> L1 penalty for the sparsity constraint, by default 0.0. Only used with 'pgd' solver.</p> </li> </ul> </li> <li> <p>verbose            : bool, optional </p> <ul> <li><p> Whether to print the status of the optimization, by default False.</p> </li> </ul> </li> </ul>"},{"location":"optimization/seminmf/#init_random_z","title":"<code>init_random_z(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A)</code>","text":"<p>Initialize the codes Z using random values (can be negative). </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Initialized codes tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Return the learned dictionary components from Semi-NMF. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Dictionary components.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/#encode","title":"<code>encode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_iter=300,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 tol=None)</code>","text":"<p>Encode the input tensor (the activations) using Semi-NMF. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor or Iterable </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> <li> <p>max_iter            : int, optional </p> <ul> <li><p> Maximum number of iterations, by default 300.</p> </li> </ul> </li> <li> <p>tol            : float, optional </p> <ul> <li><p> Tolerance value for the stopping criterion, by default the value set in the constructor.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Encoded features (the codes).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/#sanitize_np_input","title":"<code>sanitize_np_input(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Ensure the input tensor is a numpy array of shape (batch_size, dims). Convert from pytorch tensor or DataLoader if necessary. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor or Iterable </p> <ul> <li><p> Input tensor of shape (batch_size, dims).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Sanitized input tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/#fit","title":"<code>fit(self,\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0 max_iter=500)</code>","text":"<p>Fit the Semi-NMF model to the input data. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor or Iterable </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> <li> <p>max_iter            : int, optional </p> <ul> <li><p> Maximum number of iterations, by default 500.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/#init_random_d","title":"<code>init_random_d(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Z)</code>","text":"<p>Initialize the dictionary D using matrix inversion. </p> <p>Parameters</p> <ul> <li> <p>A            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, n_features).</p> </li> </ul> </li> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Codes tensor of shape (batch_size, nb_concepts).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>D            : torch.Tensor </p> <ul> <li><p> Initialized dictionary tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/#sanitize_np_codes","title":"<code>sanitize_np_codes(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Ensure the codes tensor (Z) is a numpy array of shape (batch_size, nb_concepts). Convert from pytorch tensor or DataLoader if necessary. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Encoded tensor (the codes) of shape (batch_size, nb_concepts).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Sanitized codes tensor.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"optimization/seminmf/#decode","title":"<code>decode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Z)</code>","text":"<p>Decode the input tensor (the codes) using Semi-NMF. </p> <p>Parameters</p> <ul> <li> <p>Z            : torch.Tensor </p> <ul> <li><p> Encoded tensor (the codes) of shape (batch_size, nb_concepts).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Decoded output (the activations).</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Convex and Semi-Nonnegative Matrix Factorizations by Ding, Li, and Jordan (2008).\u00a0\u21a9</p> </li> </ol>"},{"location":"saes/archetypal/","title":"Relaxed Archetypal Dictionary","text":"<p>Archetypal SAE introduces a constraint on the dictionary where each atom is formed as a convex combination of data points with an additional relaxation term. This method enhances stability and interpretability in dictionary learning, making it a robust drop-in replacement for the dictionary layer in any Sparse Autoencoder. Simply initialize the archetypal dictionary and assign it to your SAE (e.g., <code>sae.dictionary = archetypal_dict</code>).</p>"},{"location":"saes/archetypal/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nfrom overcomplete.sae.batchtopk_sae import BatchTopKSAE\nfrom overcomplete.sae.archetypal_dictionary import RelaxedArchetypalDictionary\n\n# initialize any sae\nsae = BatchTopKSAE(768, 10_000, top_k=50)\n\n# assume 'points' is a tensor of candidate data points (e.g., sampled from your dataset)\n# the original paper recommend k-means\npoints = torch.randn(1000, 768)\n\n# create our ra-sae\narchetypal_dict = RelaxedArchetypalDictionary(\n    in_dimensions=768,\n    nb_concepts=10_000,\n    points=points,\n    delta=1.0,\n)\n\n# set the SAE's dictionary with the archetypal dictionary\nsae.dictionary = archetypal_dict\n# you can now train normally your sae\n</code></pre>"},{"location":"saes/archetypal/#RelaxedArchetypalDictionary","title":"<code>RelaxedArchetypalDictionary</code>","text":"<p>Dictionary used for Relaxed Archetypal SAE (RA-SAE). </p>"},{"location":"saes/archetypal/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 in_dimensions,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 points,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 delta=1.0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 use_multiplier=True,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu')</code>","text":"<p>Parameters</p> <ul> <li> <p>in_dimensions            : int </p> <ul> <li><p> Dimensionality of the input data (e.g number of channels).</p> </li> </ul> </li> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of components/concepts in the dictionary. The dictionary is overcomplete if the number of concepts &gt; in_dimensions.</p> </li> </ul> </li> <li> <p>points            : tensors </p> <ul> <li><p> Real data points (or point in the convex hull) used to find the candidates archetypes.</p> </li> </ul> </li> <li> <p>delta            : float, optional </p> <ul> <li><p> Constraint on the relaxation term, by default 1.0.</p> </li> </ul> </li> <li> <p>use_multiplier            : bool, optional </p> <ul> <li><p> Whether to train a positive scalar to multiply the dictionary after convex combination, making the dictionary in the conical hull (and not convex hull) of the data points, by default True.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device to run the model on ('cpu' or 'cuda'), by default 'cpu'.</p> </li> </ul> </li> </ul>"},{"location":"saes/archetypal/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Reconstruct input data from latent representation. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Latent representation tensor of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, dimensions).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/archetypal/#train","title":"<code>train(self,\u00a0\u00a0\u00a0\u00a0\u00a0 mode=True)</code>","text":"<p>Hook called when switching between training and evaluation mode. We use it to fuse W, C, Relax and multiplier into a single dictionary. </p> <p>Parameters</p> <ul> <li> <p>mode            : bool, optional </p> <ul> <li><p> Whether to set the model to training mode or not, by default True.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/archetypal/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Get the dictionary. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> The dictionary tensor of shape (nb_components, dimensions).</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Archetypal-SAE by Fel et al. (2025).\u00a0\u21a9</p> </li> </ol>"},{"location":"saes/batchtopk/","title":"Batch TopK SAE","text":"<p>Batch TopK SAE is a variation of the standard Sparse Autoencoder (SAE) that enforces structured sparsity at the batch level using a global Top-K selection mechanism. Instead of selecting the K largest activations per sample, this method selects the top-K activations across the entire batch, ensuring a controlled level of sparsity.</p> <p>The architecture follows the standard SAE framework, consisting of an encoder, a decoder, and a forward method:</p> <ul> <li><code>encode</code> returns the pre-codes (<code>z_pre</code>, before thresholding) and codes (<code>z</code>) given an input (<code>x</code>).</li> <li><code>decode</code> returns a reconstructed input (<code>x_hat</code>) based on an input (<code>x</code>).</li> <li><code>forward</code> returns the pre-codes, codes, and reconstructed input.</li> </ul> <p>We strongly encourage you to check the original paper <sup>1</sup> to learn more about Batch TopK SAE.</p>"},{"location":"saes/batchtopk/#basic-usage","title":"Basic Usage","text":"<pre><code>from overcomplete import BatchTopKSAE\n\n# define a Batch TopK SAE with input dimension 768, 10k concepts\n# and top_k = 50 (for the entire batch!)\nsae = BatchTopKSAE(768, 10_000, top_k=50)\n\n# the threshold momentum is used to estimate\n# the final threshold (when in eval)\nsae = BatchTopKSAE(768, 10_000, top_k=10, threshold_momentum=0.95)\n# ... training sae\nsae = sae.eval()\n# now top_k is no longer use and instead an\n# internal threshold is used\nprint(sae.running_threshold)\n</code></pre>"},{"location":"saes/batchtopk/#BatchTopKSAE","title":"<code>BatchTopKSAE</code>","text":"<p>Batch Top-k Sparse SAE. </p>"},{"location":"saes/batchtopk/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 input_shape,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 top_k=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 threshold_momentum=0.9,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 encoder_module=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dictionary_params=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu')</code>","text":"<p>Parameters</p> <ul> <li> <p>input_shape            : int or tuple of int </p> <ul> <li><p> Dimensionality of the input data (excluding the batch dimension).</p> </li> </ul> </li> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of latent dimensions (components) of the autoencoder.</p> </li> </ul> </li> <li> <p>top_k            : int </p> <ul> <li><p> The number of activations to keep (the kth highest activation is used as threshold).</p> </li> </ul> </li> <li> <p>threshold_momentum            : float, optional </p> <ul> <li><p> Momentum for the running threshold update (default is 0.9).</p> </li> </ul> </li> <li> <p>encoder_module            : nn.Module or str, optional </p> <ul> <li><p> Custom encoder module (or its registered name). If None, a default encoder is used.</p> </li> </ul> </li> <li> <p>dictionary_params            : dict, optional </p> <ul> <li><p> Parameters that will be passed to the dictionary layer.</p><p> See DictionaryLayer for more details.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device on which to run the model (default is 'cpu').</p> </li> </ul> </li> </ul>"},{"location":"saes/batchtopk/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Perform a forward pass through the autoencoder. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>SAEOuput </p> <ul> <li><p> Return the pre_codes (z_pre), codes (z) and reconstructed input tensor (x_hat).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/batchtopk/#encode","title":"<code>encode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Encode input data and apply global top-k thresholding. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>pre_codes            : torch.Tensor </p> <ul> <li><p> The raw outputs from the encoder.</p> </li> </ul> </li> <li> <p>z            : torch.Tensor </p> <ul> <li><p> The sparse latent representation after thresholding.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/batchtopk/#decode","title":"<code>decode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Decode latent representation to reconstruct input data. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Latent representation tensor of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/batchtopk/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Return the learned dictionary. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Learned dictionary tensor of shape (nb_components, input_size).</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Batch Top-k Sparse Autoencoders by Bussmann et al. (2024).\u00a0\u21a9</p> </li> </ol>"},{"location":"saes/jumprelu/","title":"JumpReLU SAE","text":"<p>JumpReLU SAE is a variation of the standard Sparse Autoencoder (SAE) that incorporates the JumpReLU activation function to have an adaptive sparsity without shriking. This involve a learnable thresholding mechanism on each concept. As all SAEs, it include an encoder, a decoder, and a forward method.</p> <ul> <li><code>encode</code> returns the pre-codes (<code>z_pre</code>, before ReLU) and codes (<code>z</code>) given an input (<code>x</code>).</li> <li><code>decode</code> returns a reconstructed input (<code>x_hat</code>) based on an input (<code>x</code>).</li> <li><code>forward</code> returns the pre-codes, codes, and reconstructed input.</li> </ul> <p>kernel='silverman', bandwith=1e-3,</p> <p>The specificity of this architecture is that i contains 2 hyperparamter, a bandwith and a kernel. We strongly encourage you to check the original paper <sup>1</sup> to know more about JumpReLU.</p>"},{"location":"saes/jumprelu/#basic-usage","title":"Basic Usage","text":"<pre><code>from overcomplete import JumpSAE\n\n# define a JumpReLU SAE with input dimension 768 and 10k concepts\nsae = JumpSAE(768, 10_000)\n\n# adjust kernel and bandwith\nsae = JumpSAE(768, 10_000, bandwith = 1e-2,\n              kernel='silverman')\n</code></pre>"},{"location":"saes/jumprelu/#JumpSAE","title":"<code>JumpSAE</code>","text":"<p>JumpReLU Sparse Autoencoder (SAE). </p>"},{"location":"saes/jumprelu/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 input_shape,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 kernel='silverman',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 bandwith=0.001,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 encoder_module=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dictionary_params=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu')</code>","text":"<p>Parameters</p> <ul> <li> <p>input_shape            : int or tuple of int </p> <ul> <li><p> Dimensionality of the input data, do not include batch dimensions.</p><p> It is usually 1d (dim), 2d (seq length, dim) or 3d (dim, height, width).</p> </li> </ul> </li> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of components/concepts in the dictionary. The dictionary is overcomplete if the number of concepts &gt; in_dimensions.</p> </li> </ul> </li> <li> <p>kernel            : str, optional </p> <ul> <li><p> Kernel function to use in the JumpReLU activation, by default 'silverman'.</p><p> Current options are :     - 'rectangle'     - 'gaussian'     - 'triangular'     - 'cosine'     - 'epanechnikov'     - 'quartic'     - 'silverman'     - 'cauchy'.</p> </li> </ul> </li> <li> <p>bandwith            : float, optional </p> <ul> <li><p> Bandwith of the kernel, by default 1e-3.</p> </li> </ul> </li> <li> <p>encoder_module            : nn.Module or string, optional </p> <ul> <li><p> Custom encoder module, by default None.</p><p> If None, a simple Linear + BatchNorm default encoder is used.</p><p> If string, the name of the registered encoder module.</p> </li> </ul> </li> <li> <p>dictionary_params            : dict, optional </p> <ul> <li><p> Parameters that will be passed to the dictionary layer.</p><p> See DictionaryLayer for more details.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device to run the model on, by default 'cpu'.</p> </li> </ul> </li> </ul>"},{"location":"saes/jumprelu/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Perform a forward pass through the autoencoder. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>SAEOuput </p> <ul> <li><p> Return the pre_codes (z_pre), codes (z) and reconstructed input tensor (x_hat).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/jumprelu/#encode","title":"<code>encode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Encode input data to latent representation. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>pre_codes            : torch.Tensor </p> <ul> <li><p> Pre-codes tensor of shape (batch_size, nb_components) before the jump operation.</p> </li> </ul> </li> <li> <p>codes            : torch.Tensor </p> <ul> <li><p> Codes, latent representation tensor (z) of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/jumprelu/#decode","title":"<code>decode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Decode latent representation to reconstruct input data. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Latent representation tensor of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/jumprelu/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Return the learned dictionary. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Learned dictionary tensor of shape (nb_components, input_size).</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders (2024) by Rajamanoharan et al. (2024).\u00a0\u21a9</p> </li> </ol>"},{"location":"saes/topk/","title":"TopK SAE","text":"<p>TopK SAE is a variation of the standard Sparse Autoencoder (SAE) that enforces structured sparsity using a Top-K selection mechanism. This method ensures that only the K most significant activations are retained in the encoded representation, promoting interpretability and feature selection.</p> <p>The architecture follows the standard SAE framework, consisting of an encoder, a decoder, and a forward method:</p> <ul> <li><code>encode</code> returns the pre-codes (<code>z_pre</code>, before activation) and codes (<code>z</code>) given an input (<code>x</code>).</li> <li><code>decode</code> returns a reconstructed input (<code>x_hat</code>) based on an input (<code>x</code>).</li> <li><code>forward</code> returns the pre-codes, codes, and reconstructed input.</li> </ul> <p>We strongly encourage you to check the original paper <sup>1</sup> to learn more about TopK SAE.</p>"},{"location":"saes/topk/#basic-usage","title":"Basic Usage","text":"<pre><code>from overcomplete import TopKSAE\n\n# define a TopK SAE with input dimension 768, 10k concepts\nsae = TopKSAE(768, 10_000, top_k=5)\n\n# adjust the encoder module (you can also)\n# directly pass your own encoder module\nsae = TopKSAE(768, 10_000, top_k=10,\n              encoder_module='mlp_bn_1')\n</code></pre>"},{"location":"saes/topk/#TopKSAE","title":"<code>TopKSAE</code>","text":"<p>Top-k Sparse SAE. </p>"},{"location":"saes/topk/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 input_shape,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 top_k=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 encoder_module=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dictionary_params=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu')</code>","text":"<p>Parameters</p> <ul> <li> <p>input_shape            : int or tuple of int </p> <ul> <li><p> Dimensionality of the input data, do not include batch dimensions.</p><p> It is usually 1d (dim), 2d (seq length, dim) or 3d (dim, height, width).</p> </li> </ul> </li> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of components/concepts in the dictionary. The dictionary is overcomplete if the number of concepts &gt; in_dimensions.</p> </li> </ul> </li> <li> <p>top_k            : int, optional </p> <ul> <li><p> Number of top activations to keep in the latent representation, by default n_components // 10 (sparsity of 90%).</p> </li> </ul> </li> <li> <p>encoder_module            : nn.Module or string, optional </p> <ul> <li><p> Custom encoder module, by default None.</p><p> If None, a simple Linear + BatchNorm default encoder is used.</p><p> If string, the name of the registered encoder module.</p> </li> </ul> </li> <li> <p>dictionary_params            : dict, optional </p> <ul> <li><p> Parameters that will be passed to the dictionary layer.</p><p> See DictionaryLayer for more details.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device to run the model on, by default 'cpu'.</p> </li> </ul> </li> </ul>"},{"location":"saes/topk/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Perform a forward pass through the autoencoder. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>SAEOuput </p> <ul> <li><p> Return the pre_codes (z_pre), codes (z) and reconstructed input tensor (x_hat).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/topk/#encode","title":"<code>encode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Encode input data to latent representation. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>pre_codes            : torch.Tensor </p> <ul> <li><p> Pre-codes tensor of shape (batch_size, nb_components) before the relu and top-k operation.</p> </li> </ul> </li> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Codes, latent representation tensor (z) of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/topk/#decode","title":"<code>decode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Decode latent representation to reconstruct input data. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Latent representation tensor of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/topk/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Return the learned dictionary. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Learned dictionary tensor of shape (nb_components, input_size).</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Scaling and Evaluating Sparse Autoencoders by Gao et al. (2024).\u00a0\u21a9</p> </li> </ol>"},{"location":"saes/vanilla/","title":"Vanilla SAE","text":"<p>The most basic SAE. It consists of an encoder and a decoder. The decoder is simply a dictionary, while the encoder can be configured. By default, it is a linear module with bias and ReLU activation. All SAEs include an encoder, a decoder, and a forward method.</p> <ul> <li><code>encode</code> returns the pre-codes (<code>z_pre</code>, before ReLU) and codes (<code>z</code>) given an input (<code>x</code>).</li> <li><code>decode</code> returns a reconstructed input (<code>x_hat</code>) based on an input (<code>x</code>).</li> <li><code>forward</code> returns the pre-codes, codes, and reconstructed input.</li> </ul>"},{"location":"saes/vanilla/#basic-usage","title":"Basic Usage","text":"<pre><code>from overcomplete import SAE\n\n# Define a basic SAE where input dimension is 768, with 10k concepts\n# Using a simple linear encoding\nsae = SAE(768, 10_000)\n\n# Define a more complex SAE with batch normalization in the encoder\n# The dictionary is normalized on the L1 ball instead of L2\nsae = SAE(768, 10_000, encoder_module='mlp_bn_1',\n          dictionary_params={'normalization': 'l1'})\n</code></pre>"},{"location":"saes/vanilla/#SAE","title":"<code>SAE</code>","text":"<p>Sparse Autoencoder (SAE) model for dictionary learning. </p>"},{"location":"saes/vanilla/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 input_shape,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_concepts,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 encoder_module=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dictionary_params=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu')</code>","text":"<p>Parameters</p> <ul> <li> <p>input_shape            : int or tuple of int </p> <ul> <li><p> Dimensionality of the input data, do not include batch dimensions.</p><p> It is usually 1d (dim), 2d (seq length, dim) or 3d (dim, height, width).</p> </li> </ul> </li> <li> <p>nb_concepts            : int </p> <ul> <li><p> Number of components/concepts in the dictionary. The dictionary is overcomplete if the number of concepts &gt; in_dimensions.</p> </li> </ul> </li> <li> <p>encoder_module            : nn.Module or string, optional </p> <ul> <li><p> Custom encoder module, by default None.</p><p> If None, a simple Linear + BatchNorm default encoder is used.</p><p> If string, the name of the registered encoder module.</p> </li> </ul> </li> <li> <p>dictionary_params            : dict, optional </p> <ul> <li><p> Parameters that will be passed to the dictionary layer.</p><p> See DictionaryLayer for more details.</p> </li> </ul> </li> <li> <p>device            : str, optional </p> <ul> <li><p> Device to run the model on, by default 'cpu'.</p> </li> </ul> </li> </ul>"},{"location":"saes/vanilla/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Perform a forward pass through the autoencoder. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>SAEOuput </p> <ul> <li><p> Return the pre_codes (z_pre), codes (z) and reconstructed input tensor (x_hat).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/vanilla/#encode","title":"<code>encode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x)</code>","text":"<p>Encode input data to latent representation. </p> <p>Parameters</p> <ul> <li> <p>x            : torch.Tensor </p> <ul> <li><p> Input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>pre_codes            : torch.Tensor </p> <ul> <li><p> Pre-codes tensor of shape (batch_size, nb_components), before the activation function.</p> </li> </ul> </li> <li> <p>codes            : torch.Tensor </p> <ul> <li><p> Codes, latent representation tensor (z) of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/vanilla/#decode","title":"<code>decode(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 z)</code>","text":"<p>Decode latent representation to reconstruct input data. </p> <p>Parameters</p> <ul> <li> <p>z            : torch.Tensor </p> <ul> <li><p> Latent representation tensor of shape (batch_size, nb_components).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Reconstructed input tensor of shape (batch_size, input_size).</p> </li> </ul> </li> </ul> <p></p>"},{"location":"saes/vanilla/#get_dictionary","title":"<code>get_dictionary(self)</code>","text":"<p>Return the learned dictionary. </p> <p>Return</p> <ul> <li> <p>torch.Tensor </p> <ul> <li><p> Learned dictionary tensor of shape (nb_components, input_size).</p> </li> </ul> </li> </ul> <p></p>"}]}